{
  "cells": [
    {
      "cell_type": "code",
      "id": "tUwGiEQG7UwkNiA6crbwAV8P",
      "metadata": {
        "tags": [],
        "id": "tUwGiEQG7UwkNiA6crbwAV8P"
      },
      "source": [
        "!pip install --quiet google-cloud-bigquery google-cloud-aiplatform --quiet"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"qwiklabs-gcp-04-c85702fbf940\"\n",
        "LOCATION = \"global\"\n",
        "EMBEDDED_TABLE_ID = f\"{PROJECT_ID}.challenge2.faq_embedded\"\n",
        "EMBEDDED_MODEL_ID = f\"{PROJECT_ID}.challenge2.Embeddings\"\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "def query_vector_db(question):\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "     query.query,\n",
        "    base.content\n",
        "    FROM VECTOR_SEARCH(\n",
        "      TABLE `{EMBEDDED_TABLE_ID}`,\n",
        "      'ml_generate_embedding_result',\n",
        "      (\n",
        "        SELECT\n",
        "          ml_generate_embedding_result AS embedding,\n",
        "          content AS query\n",
        "        FROM ML.GENERATE_EMBEDDING(\n",
        "          MODEL `{EMBEDDED_MODEL_ID}`,\n",
        "          (SELECT '{question}' AS content)\n",
        "        )\n",
        "      ),\n",
        "      top_k => 3,\n",
        "      options => '{{\"fraction_lists_to_search\": 1.0}}'\n",
        "    ) AS result\n",
        "    \"\"\"\n",
        "    return bq_client.query(query).to_dataframe()\n"
      ],
      "metadata": {
        "id": "4-0sFZOt9QzX"
      },
      "id": "4-0sFZOt9QzX",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.generative_models import GenerativeModel\n",
        "from vertexai.preview.generative_models import GenerativeModel, ChatSession, SafetySetting\n",
        "\n",
        "system_instruction = \"\"\"\n",
        "ROLE:\n",
        "You are a FAQ chat bot\n",
        "\n",
        "TASK:\n",
        "Answer questions based on the given context\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Do not answer any question unrelated to the given context\n",
        "- Give only the answer and do not explain anything.\n",
        "- Respond 'I can't help with It' if asked about anything that you are not allowed to respond even part of it has valid query\n",
        "\"\"\"\n",
        "\n",
        "model = GenerativeModel(\n",
        "    model_name=\"gemini-2.0-flash-001\",\n",
        "    system_instruction=system_instruction,\n",
        "    safety_settings=[\n",
        "        SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
        "        SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
        "        SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
        "        SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
        "    ]\n",
        ")\n",
        "chat = model.start_chat()\n",
        "\n",
        "def chat_bot(prompt):\n",
        "    results = query_vector_db(prompt)\n",
        "    context = \"\\n\\n\".join([f\"{row['content']}\" for _,row in results.iterrows()])\n",
        "    prompt = f\"You are a faq service. Use the context:{context} to answer: {prompt}\"\n",
        "    code = chat.send_message(prompt)\n",
        "    return code.text\n",
        "\n",
        "def chat_user():\n",
        "    while True:\n",
        "        prompt = input(\"User: \")\n",
        "        if prompt.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Chat closed.\")\n",
        "            break\n",
        "        print(\"FAQ assistant:\", chat_bot(prompt))\n",
        "\n",
        "chat_user()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y74dFL8z91Cd",
        "outputId": "7c4e8ede-1dd0-45e6-c016-1b7465459a29"
      },
      "id": "y74dFL8z91Cd",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What is the population of Aurora Bay?\n",
            "FAQ assistant: Aurora Bay has a population of approximately 3,200 residents, although it can fluctuate seasonally due to temporary fishing and tourism workforces.\n",
            "\n",
            "User: When was it founded\n",
            "FAQ assistant: Aurora Bay was founded in 1901 by a group of fur traders who recognized the region’s strategic coastal location.\n",
            "\n",
            "User: exit\n",
            "Chat closed.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-04-92dc83e6cf24 (Jun 16, 2025, 2:27:00 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}